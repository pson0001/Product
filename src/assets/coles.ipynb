{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0927c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin' # model visualisation -> \n",
    "# add the binaries to path! \n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./logs\")\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import concatenate, Input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4568d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 8772992878236577955\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 7804551168\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 6686604937781567478\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set / double check for cuDNN enabled device \n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42074176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util functions\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1- load_to_memory : : input filepaths to return data. Main source is CSVs will use pd.DataFrame types.\n",
    "                    : this method also executes a log(x) + 1 transform to the unit_sales.\n",
    "                    : predictions are done on this basis later to be transformed by the inverse of\n",
    "                    : this function. \n",
    "                    : type conversions are also executed to data\n",
    "\n",
    "\n",
    "2- sales_and_promotion_table : : restructuring and setting up the raw data to extract features from Returns 3 dfs: \n",
    "                               \n",
    "                               : Y feature vector set up schema: Populated by log(sales) + 1 figures\n",
    "                               : multilvl index of store_nbr|item_nbr\n",
    "                               : all dates are the columns\n",
    "                               \n",
    "                                                     DATE-1 | DATE-1 | DATE-1 | ...\n",
    "                               -------------------------------------------------------\n",
    "                               |store_nbr|item_nbr||   \n",
    "                                    1        1     |\n",
    "                                             2     |\n",
    "                                             3     |\n",
    "                                             .     |\n",
    "                                             .     |\n",
    "                                             .     |\n",
    "                                    2        1     |\n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                               : X feature vector set up:\n",
    "                               : Promotion Look up: used to mine promotion features. Features that look ahead, and behind.\n",
    "                            \n",
    "    \n",
    "3- sales_meta : : put in store meta_data ... main output is the first position output. This will be the dataframe that the\n",
    "                  get_features() function will mainly extract features from:\n",
    "                  \n",
    "                  store_nbr| item_nbr| date | unit_sales | day | day_of_week | is_no_sale |family | class | city | state |type | cluster\n",
    "                  \n",
    "                  \n",
    "                  \n",
    "                  \n",
    "4- get_features : :\n",
    "\n",
    "              #############################:  basic stats of sales figures store_nbr & item_nbr pair :\n",
    "              last                         : \"sales value of day prior given store_nbr & item_nbr pair\" \n",
    "              mean_3                       : \"mean sales value of the last 3 days given store_nbr & item_nbr pair\"\n",
    "              mean_7                       : \"mean sales value of the last 7 days given store_nbr & item_nbr pair\"\n",
    "              mean_14                      : \"mean sales value of the last 14 days given store_nbr & item_nbr pair\"\n",
    "              mean_28                      : \"mean sales value of the last 28 days given store_nbr & item_nbr pair\"\n",
    "              mean_60                      : \"mean sales value of the last 60 days given store_nbr & item_nbr pair\"\n",
    "              mean_90                      : \"mean sales value of the last 90 days given store_nbr & item_nbr pair\"\n",
    "              mean_365                     : \"mean sales value of the last 365 days given store_nbr & item_nbr pair\"\n",
    "              mean_diff_7_28               : \"variance of mean 7 - 28 given store_nbr & item_nbr pair\"\n",
    "              mean_diff_14_60              : \"variance of mean 14 - 60 given store_nbr & item_nbr pair\"\n",
    "              mean_diff_28_90              : \"variance of mean 28 - 90 given store_nbr & item_nbr pair\"\n",
    "              mean_no_sale_in_7            : \"no sale mean count - 7 days given store_nbr & item_nbr pair\"\n",
    "              mean_no_sale_in_28           : \"no sale mean count - 28 days given store_nbr & item_nbr pair\"\n",
    "              mean_no_sale_in_90           : \"no sale mean count - 90 days given store_nbr & item_nbr pair\"\n",
    "              mean_no_sale_diff_7_28       : \"variance of no sale mean count - 7 - 28 days given store_nbr & item_nbr pair\"\n",
    "              mean_no_sale_diff_28_90      : \"variance of no sale mean count - 28 - 90 days given store_nbr & item_nbr pair\"\n",
    "              \n",
    "              #############################:  promotion stats & day specific sales features given store_nbr & item_nbr pair :\n",
    "              avg_promo_7                  : \"avg number of days an item was on promo in the last 7 days given store_nbr & item_nbr pair\"      \n",
    "              avg_promo_14                 : \"avg number of days an item was on promo in the last 14 days given store_nbr & item_nbr pair\"\n",
    "              avg_promo_28                 : \"avg number of days an item was on promo in the last 28 days given store_nbr & item_nbr pair\"\n",
    "              avg_promo_90                 : \"avg number of days an item was on promo in the last 90 days given store_nbr & item_nbr pair\"\n",
    "              avg_promo_365                : \"avg number of days an item was on promo in the last 365 days given store_nbr & item_nbr pair\"\n",
    "              mean_day_of_week_7           : \"mean sales for day of the wk in last 7 days given store_nbr & item_nbr pair\"\n",
    "              mean_day_of_week_14          : \"mean sales for day of the wk in last 14 days given store_nbr & item_nbr pair\"\n",
    "              mean_day_of_week_21          : \"mean sales for day of the wk in last 21 days given store_nbr & item_nbr pair\"\n",
    "              mean_day_of_week_28          : \"mean sales for day of the wk in last 28 days given store_nbr & item_nbr pair\"\n",
    "              mean_day_in_90               : \"mean sales for day of the month in last 90 days given store_nbr & item_nbr pair\"\n",
    "              mean_day_in_365              : \"mean sales for day of the month in last 365 days given store_nbr & item_nbr pair\"\n",
    "              promo                        : \"looking forward -> promo states of next 16 given store_nbr & item_nbr pair\"\n",
    "              \n",
    "              #############################:  metadata given store_nbr & item_nbr pair :\n",
    "              family                       \n",
    "              class\n",
    "              perishable\n",
    "              city\n",
    "              state\n",
    "              type\n",
    "              cluster\n",
    "              day_of_week\n",
    "              day\n",
    "\n",
    "\n",
    "5- get_y : : returns target values, i.e. sales\n",
    "\n",
    "6- nwrmsle : : given 2 vectors returns Normalized Weighted Root Mean Squared Logarithmic Error\n",
    "\n",
    "7- _features : : returns set of features to feed into model. This function also returns features that are 'day' sensitive.\n",
    "                 eg: given we are predicting for the 17th of a Monday, features are retrieved for Mondays and 17ths of the month.\n",
    "\n",
    "8- get_model : : returns the keras implementation of the graph  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def load_to_memory(filepath:str,is_test = False):\n",
    "    \n",
    "    if not is_test:\n",
    "        \n",
    "        data = pd.read_csv(filepath,\n",
    "                       dtype={ \n",
    "                              'date': str,\n",
    "                              'store_nbr': int,\n",
    "                              'item':int,\n",
    "                              'unit_sales':'float32'\n",
    "                       })\n",
    "        \n",
    "        data.onpromotion = data.onpromotion.fillna(0).astype(int)\n",
    "        \n",
    "        # transform unit_sales if sales>0 else clip to 0 (this operation turns all return activity to 0 units sold)\n",
    "        data.unit_sales = data.unit_sales.apply(lambda x: np.log1p(float(x)) if float(x) > 0 else 0)\n",
    "        data.unit_sales = data.unit_sales.astype('float32')\n",
    "        data = data.drop(['id'], axis=1)\n",
    "    if is_test:\n",
    "        \n",
    "        data = pd.read_csv(filepath, usecols=[0, 1, 2, 3, 4],\n",
    "                       dtype={ \n",
    "                              'date': str,\n",
    "                              'store_nbr': int,\n",
    "                              'item':int \n",
    "                       })\n",
    "        data.onpromotion = data.onpromotion.fillna(0).astype(int)\n",
    "        \n",
    "\n",
    "    # date column dtype str -> datetime64\n",
    "    data.date = pd.to_datetime(data.date, format='%Y-%m-%d')\n",
    "    # convert bool to int\n",
    "    data.onpromotion = data.onpromotion.astype('int')\n",
    "    # the promotions are recorded starting from April - 2014 ... we ignore training samples prior to period\n",
    "    return data[data.date >= pd.to_datetime('2014-04-01', format='%Y-%m-%d')]\n",
    "    \n",
    "\n",
    "def sales_and_promotion_table(_train_stack, _test_stack, key_array: list) -> tuple:\n",
    "    \n",
    "    # sales restructure dataframe to host multindex & time series nature from 2014-04 till end of data & fixing missing values if any\n",
    "    sales = _train_stack.set_index(key_array).unit_sales.unstack().fillna(0)\n",
    "    # create a feature set\n",
    "    sales_features = sales.stack().rename('unit_sales').reset_index()\n",
    "    # check and fill empty dates, will be used for labels\n",
    "    sales = sales.reindex(pd.date_range('2014-04-01', '2017-08-15'), axis=1).fillna(0)\n",
    "    # promotions \n",
    "    training_p = _train_stack.set_index(key_array)[[\"onpromotion\"]].unstack().fillna(0)\n",
    "    test_p = _test_stack.set_index(key_array)[[\"onpromotion\"]].unstack().fillna(0)\n",
    "    # restructure the level of DF to have columns of DF in top level\n",
    "    training_p.columns = training_p.columns.get_level_values(1)\n",
    "    test_p.columns = test_p.columns.get_level_values(1)\n",
    "    # reindex\n",
    "    test_p = test_p.reindex(training_p.index).fillna(0)\n",
    "    # putting promotional data of train and test together to look ahead for poromotions while predicting & fix missing values if any\n",
    "    promotions = pd.concat([training_p, test_p], axis=1).reindex(pd.date_range('2014-04-01', '2017-08-31'), axis=1).fillna(0)\n",
    "    \n",
    "    return sales, sales_features, promotions\n",
    "\n",
    "def sales_meta(df, item_doc_dir : str, store_doc_dir : str) -> tuple :\n",
    "    \n",
    "    # load memory\n",
    "    \n",
    "    items = pd.read_csv(item_doc_dir,\n",
    "                    dtype={'item_nbr': 'int32', \n",
    "                           'class': int, \n",
    "                           'perishable': int})\n",
    "\n",
    "\n",
    "    stores = pd.read_csv(store_doc_dir,\n",
    "                    dtype={'store_nbr': int,\n",
    "                           'cluster': int})\n",
    "    \n",
    "    df['day'] = df.date.dt.day\n",
    "    df['day_of_week'] = df.date.dt.dayofweek\n",
    "    df['is_no_sale'] = (df.unit_sales==0).astype(int) # turn bool to int\n",
    "    df = pd.merge(pd.merge(df, items.drop('perishable', axis=1)), stores)\n",
    "    \n",
    "    return df, pd.merge(pd.merge(df[['store_nbr', 'item_nbr']].drop_duplicates(), items), stores)    \n",
    "\n",
    "\n",
    "def get_features(feature_date):\n",
    "    \n",
    "    # feature sets\n",
    "    #####################################################################################\n",
    "\n",
    "\n",
    "    # get last sales (key: store_nbr & item_nbr) --- look behind a day from prediction day\n",
    "    temp_last_sales = sales_set_x[(sales_set_x.date >= (pd.to_datetime(feature_date) - pd.Timedelta(days=1))) &\n",
    "                       (sales_set_x.date <= (pd.to_datetime(feature_date) - pd.Timedelta(days=1)))]\n",
    "    temp_last_sales = temp_last_sales.groupby(['store_nbr', 'item_nbr'])['unit_sales'].agg(['last']).reset_index()\n",
    "\n",
    "\n",
    "    # get avg last sales (key: store_nbr & item_nbr) --- look 3 | 7 | 14 | 28 | 90 | 365 days behind prediction day\n",
    "\n",
    "    for i in [3,7,14,28,60,90,365]:\n",
    "\n",
    "        temp_avg_run = sales_set_x[(sales_set_x.date >= (pd.to_datetime(feature_date) - pd.Timedelta(days=i))) &\n",
    "                       (sales_set_x.date <= (pd.to_datetime(feature_date) - pd.Timedelta(days=1)))].groupby(['store_nbr', 'item_nbr'])['unit_sales'].agg(['mean']).reset_index()\n",
    "        temp_avg_run.columns = ['store_nbr', 'item_nbr', 'mean_{}'.format(i)]\n",
    "        temp_last_sales = pd.merge(temp_last_sales, temp_avg_run)\n",
    "\n",
    "    print('DONE: last sales and avg sales calculated for {}'.format(feature_date))\n",
    "\n",
    "    # difference in means for different wks \n",
    "\n",
    "    temp_last_sales['mean_diff_7_28'] = temp_last_sales.mean_7 - temp_last_sales.mean_28\n",
    "    temp_last_sales['mean_diff_14_60'] = temp_last_sales.mean_14 - temp_last_sales.mean_60\n",
    "    temp_last_sales['mean_diff_28_90'] = temp_last_sales.mean_28 - temp_last_sales.mean_90\n",
    "\n",
    "    print('DONE: avg difference in sales calculated for {}'.format(feature_date))\n",
    "\n",
    "\n",
    "    # mean sales by day_of_week\n",
    "\n",
    "    for i in [7,14,21,28]:\n",
    "\n",
    "\n",
    "        temp_avg_run = sales_set_x[(sales_set_x.date >= (pd.to_datetime(feature_date) - pd.Timedelta(days=i))) &\n",
    "                       (sales_set_x.date <= (pd.to_datetime(feature_date) - pd.Timedelta(days=1)))].groupby(['store_nbr', 'item_nbr', 'day_of_week'])['unit_sales'].agg(['mean']).reset_index()\n",
    "        temp_avg_run.columns = ['store_nbr', 'item_nbr', 'day_of_week', 'mean_day_of_week_{}_'.format(i)]\n",
    "        # stack by day of week to avoid the categorical in the fields\n",
    "        temp_avg_run = temp_avg_run.set_index(['store_nbr', 'item_nbr', 'day_of_week']).unstack()\n",
    "        temp_avg_run.columns = temp_avg_run.columns.get_level_values(0) + temp_avg_run.columns.get_level_values(1).astype(str)\n",
    "        temp_avg_run = temp_avg_run.reset_index()\n",
    "        temp_last_sales = pd.merge(temp_last_sales, temp_avg_run)\n",
    "\n",
    "\n",
    "    print('DONE: avg sales for each day of week calculated for {}'.format(feature_date))\n",
    "\n",
    "\n",
    "    # mean sales by day of the wk -  per quarter (90) and year (365) \n",
    "\n",
    "    for i in [90,365]:\n",
    "\n",
    "\n",
    "        temp_avg_run = sales_set_x[(sales_set_x.date >= (pd.to_datetime(feature_date) - pd.Timedelta(days=i))) &\n",
    "                       (sales_set_x.date <= (pd.to_datetime(feature_date) - pd.Timedelta(days=1)))].groupby(['store_nbr', 'item_nbr', 'day'])['unit_sales'].agg(['mean']).reset_index()\n",
    "        temp_avg_run.columns = ['store_nbr', 'item_nbr', 'day', 'mean_day_in_{}_'.format(i)]\n",
    "        # stack by day of week to avoid the categorical in the fields\n",
    "        temp_avg_run = temp_avg_run.set_index(['store_nbr', 'item_nbr', 'day']).unstack()\n",
    "        temp_avg_run.columns = temp_avg_run.columns.get_level_values(0) + temp_avg_run.columns.get_level_values(1).astype(str)\n",
    "        temp_avg_run = temp_avg_run.reset_index()\n",
    "        temp_last_sales = pd.merge(temp_last_sales, temp_avg_run)\n",
    "\n",
    "    print('DONE: avg sales for each day calculated for a quarter and year for {}'.format(feature_date))\n",
    "\n",
    "\n",
    "    # mean of zero sales \n",
    "\n",
    "    for i in [7, 28, 90]:\n",
    "\n",
    "\n",
    "        temp_avg_run = sales_set_x[(sales_set_x.date >= (pd.to_datetime(feature_date) - pd.Timedelta(days=i))) &\n",
    "                       (sales_set_x.date <= (pd.to_datetime(feature_date) - pd.Timedelta(days=1)))].groupby(['store_nbr', 'item_nbr'])['is_no_sale'].agg(['mean']).reset_index()\n",
    "        temp_avg_run.columns = ['store_nbr', 'item_nbr', 'mean_no_sale_in_{}'.format(i)]\n",
    "        temp_last_sales = pd.merge(temp_last_sales, temp_avg_run)\n",
    "\n",
    "    print('DONE: avg count of item not sold in a store for {}'.format(feature_date))\n",
    "\n",
    "    # difference in means for different wks no sales\n",
    "\n",
    "    temp_last_sales['mean_no_sale_diff_7_28'] = temp_last_sales.mean_no_sale_in_7 - temp_last_sales.mean_no_sale_in_28\n",
    "    temp_last_sales['mean_no_sale_diff_28_90'] = temp_last_sales.mean_no_sale_in_28 - temp_last_sales.mean_no_sale_in_90\n",
    "\n",
    "    print('DONE: no sales diff in for {}'.format(feature_date))\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "\n",
    "    # promotions\n",
    "\n",
    "    # LOOK behind! for mean promotion data\n",
    "\n",
    "    for i in [7, 14, 28,90, 365]:\n",
    "\n",
    "\n",
    "        temp_avg_run = promotions_set[pd.date_range(pd.to_datetime(feature_date) - pd.Timedelta(days=i), periods=i, freq='D')].mean(axis=1).values\n",
    "        temp_avg_run = pd.DataFrame(temp_avg_run, index=promotions_set.index).reset_index()\n",
    "        temp_avg_run.columns = ['store_nbr', 'item_nbr', 'avg_promo_{}'.format(i)]\n",
    "        temp_last_sales = pd.merge(temp_last_sales,temp_avg_run)\n",
    "\n",
    "    print('DONE: past promo avgs for {}'.format(feature_date))\n",
    "\n",
    "\n",
    "    # LOOK ahead! we are predicting for 16 time steps forward\n",
    "\n",
    "    for i in range (16):\n",
    "\n",
    "        temp_last_sales[\"promo_{}\".format(i)] = promotions_set[pd.to_datetime(feature_date) + pd.Timedelta(days=i)].values \n",
    "\n",
    "    print('DONE: future promo data for {}'.format(feature_date))\n",
    "    print('##############################')\n",
    "    print('\\n')\n",
    "    print('##############################')\n",
    "    return temp_last_sales\n",
    "\n",
    "\n",
    "def get_y(df, _date):\n",
    "    return df[pd.date_range(_date, periods=16)]   \n",
    "\n",
    "\n",
    "\n",
    "def nwrmsle(yval, ypred, weights=None):\n",
    "    \n",
    "    # custom loss ... will train on custom MSE ... \n",
    "    return np.sqrt(mean_squared_error(np.log(1+yval), np.log(1+ypred), sample_weight=weights))\n",
    "\n",
    "\n",
    "def _features(df0, lag, y0=None):\n",
    "    \n",
    "    # range we want to predict for #\n",
    "    test_range = pd.date_range('2017-08-16', '2017-08-31')\n",
    "\n",
    "    if y0 is not None:\n",
    "        date = pd.to_datetime(y0.columns[lag])\n",
    "    else:\n",
    "        date = test_range[lag]\n",
    "\n",
    "    # main detection features #\n",
    "    df = df0[['last', 'mean_3', 'mean_7', 'mean_14', 'mean_28', 'mean_60', 'mean_90', 'mean_365',\n",
    "              'mean_diff_7_28', 'mean_diff_14_60', 'mean_diff_28_90','mean_no_sale_in_7', 'mean_no_sale_in_28', 'mean_no_sale_in_90',\n",
    "              'mean_no_sale_diff_7_28', 'mean_no_sale_diff_28_90','avg_promo_7', 'avg_promo_14', 'avg_promo_28', 'avg_promo_90','avg_promo_365']]\n",
    "    \n",
    "    # for whatever day - dayofweek we are on ... we take take that day's features #\n",
    "    day = date.day\n",
    "    dow = date.dayofweek    \n",
    "    ###############################################################################\n",
    "    df['mean_day_of_week_7']  = df0['mean_day_of_week_7_%d' % dow]\n",
    "    df['mean_day_of_week_14'] = df0['mean_day_of_week_14_%d' % dow]\n",
    "    df['mean_day_of_week_21'] = df0['mean_day_of_week_21_%d' % dow]\n",
    "    df['mean_day_of_week_28'] = df0['mean_day_of_week_28_%d' % dow]\n",
    "    df['mean_day_in_90'] = df0['mean_day_in_90_%d' % day]\n",
    "    df['mean_day_in_365'] = df0['mean_day_in_365_%d' % day]\n",
    "    df['promo'] = df0['promo_%d' % lag]\n",
    "    df['promo_mean'] = df0[['promo_0', 'promo_1', 'promo_2', 'promo_3', 'promo_4', 'promo_5', \n",
    "                            'promo_6', 'promo_7', 'promo_8', 'promo_9', 'promo_10', 'promo_11', \n",
    "                            'promo_12', 'promo_13', 'promo_14', 'promo_15']].mean(axis=1)\n",
    "    ###############################################################################\n",
    "    # embedding layers #\n",
    "    df['family'] = items2['family'].values\n",
    "    df['class'] = items2['class'].values\n",
    "    df['perishable'] = items2['perishable'].values\n",
    "    df['city'] = stores2['city'].values\n",
    "    df['state'] = stores2['state'].values\n",
    "    df['type'] = stores2['type'].values\n",
    "    df['cluster'] = stores2['cluster'].values\n",
    "    df['day_of_week'] = dow\n",
    "    df['day'] = day\n",
    "    ###############################################################################\n",
    "    df = df.reset_index()\n",
    "\n",
    "    if y0 is not None:\n",
    "        y_i = y0.iloc[:,lag].rename('y').to_frame()\n",
    "        y_i['date'] = date\n",
    "        y_i = y_i.reset_index().set_index(['store_nbr', 'item_nbr', 'date']).squeeze()\n",
    "        return df, y_i\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "    \n",
    "def get_model(input_num_shape):\n",
    "\n",
    "    input_num = Input(shape=(input_num_shape,), dtype='float32', name='input_num')\n",
    "    input_store = Input(shape=(1,), dtype='int32', name='input_store')\n",
    "    input_item = Input(shape=(1,), dtype='int32', name='input_item')\n",
    "    input_family = Input(shape=(1,), dtype='int32', name='input_family')\n",
    "    input_city = Input(shape=(1,), dtype='int32', name='input_city')\n",
    "    input_state = Input(shape=(1,), dtype='int32', name='input_state')\n",
    "    input_type = Input(shape=(1,), dtype='int32', name='input_type')\n",
    "    input_cluster = Input(shape=(1,), dtype='int32', name='input_cluster')\n",
    "    input_dow = Input(shape=(1,), dtype='int32', name='input_dow')\n",
    "    input_day = Input(shape=(1,), dtype='int32', name='input_day')\n",
    "\n",
    "    embedding_store = Embedding(input_dim=val.reset_index()['store_nbr'].nunique(), output_dim=5, input_length=1)(input_store)\n",
    "    embedding_store = Flatten()(embedding_store)\n",
    "    embedding_item = Embedding(input_dim=val.reset_index()['item_nbr'].nunique(), output_dim=10, input_length=1)(input_item)\n",
    "    embedding_item = Flatten()(embedding_item)\n",
    "    embedding_family = Embedding(input_dim=_items['family'].nunique(), output_dim=5, input_length=1)(input_family)\n",
    "    embedding_family = Flatten()(embedding_family)\n",
    "    embedding_city = Embedding(input_dim=_stores['city'].nunique(), output_dim=5, input_length=1)(input_city)\n",
    "    embedding_city = Flatten()(embedding_city)\n",
    "    embedding_state = Embedding(input_dim=_stores['state'].nunique(), output_dim=5, input_length=1)(input_state)\n",
    "    embedding_state = Flatten()(embedding_state)\n",
    "    embedding_type = Embedding(input_dim=_stores['type'].nunique(), output_dim=3, input_length=1)(input_type)\n",
    "    embedding_type = Flatten()(embedding_type)\n",
    "    embedding_cluster = Embedding(input_dim=_stores['cluster'].nunique(), output_dim=5, input_length=1)(input_cluster)\n",
    "    embedding_cluster = Flatten()(embedding_cluster)\n",
    "    embedding_dow = Embedding(input_dim=7, output_dim=5, input_length=1)(input_dow)\n",
    "    embedding_dow = Flatten()(embedding_dow)\n",
    "    embedding_day = Embedding(input_dim=31, output_dim=5, input_length=1)(input_day)\n",
    "    embedding_day = Flatten()(embedding_day)\n",
    "    \n",
    "    features = [input_num, embedding_store, embedding_item, embedding_family, embedding_city,\n",
    "               embedding_state, embedding_type, embedding_cluster, embedding_dow, embedding_day]\n",
    "    net = concatenate(features)\n",
    "    net = Dense(1000, kernel_initializer = 'he_normal', activation='relu')(net)\n",
    "    net = Dense(500, kernel_initializer = 'he_normal', activation='relu')(net)\n",
    "    net = Dense(1, kernel_initializer = 'he_normal', activation='linear')(net)\n",
    "    inputs = [input_num, input_store, input_item, input_family, input_city,\n",
    "             input_state, input_type, input_cluster, input_dow, input_day]\n",
    "    model = Model(inputs=inputs, outputs=[net])\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a9acf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loded training to memory, memory use: 3.738218004 GB\n",
      "loded test to memory, memory use: 0.13481856 GB\n",
      "DONE: last sales and avg sales calculated for 2017-07-05 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-07-05 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-07-05 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-07-05 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-07-05 00:00:00\n",
      "DONE: no sales diff in for 2017-07-05 00:00:00\n",
      "DONE: past promo avgs for 2017-07-05 00:00:00\n",
      "DONE: future promo data for 2017-07-05 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-06-28 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-06-28 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-06-28 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-06-28 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-06-28 00:00:00\n",
      "DONE: no sales diff in for 2017-06-28 00:00:00\n",
      "DONE: past promo avgs for 2017-06-28 00:00:00\n",
      "DONE: future promo data for 2017-06-28 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-06-21 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-06-21 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-06-21 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-06-21 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-06-21 00:00:00\n",
      "DONE: no sales diff in for 2017-06-21 00:00:00\n",
      "DONE: past promo avgs for 2017-06-21 00:00:00\n",
      "DONE: future promo data for 2017-06-21 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-06-14 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-06-14 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-06-14 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-06-14 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-06-14 00:00:00\n",
      "DONE: no sales diff in for 2017-06-14 00:00:00\n",
      "DONE: past promo avgs for 2017-06-14 00:00:00\n",
      "DONE: future promo data for 2017-06-14 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-06-07 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-06-07 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-06-07 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-06-07 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-06-07 00:00:00\n",
      "DONE: no sales diff in for 2017-06-07 00:00:00\n",
      "DONE: past promo avgs for 2017-06-07 00:00:00\n",
      "DONE: future promo data for 2017-06-07 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-05-31 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-05-31 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-05-31 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-05-31 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-05-31 00:00:00\n",
      "DONE: no sales diff in for 2017-05-31 00:00:00\n",
      "DONE: past promo avgs for 2017-05-31 00:00:00\n",
      "DONE: future promo data for 2017-05-31 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-05-24 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-05-24 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-05-24 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-05-24 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-05-24 00:00:00\n",
      "DONE: no sales diff in for 2017-05-24 00:00:00\n",
      "DONE: past promo avgs for 2017-05-24 00:00:00\n",
      "DONE: future promo data for 2017-05-24 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-05-17 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-05-17 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-05-17 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-05-17 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-05-17 00:00:00\n",
      "DONE: no sales diff in for 2017-05-17 00:00:00\n",
      "DONE: past promo avgs for 2017-05-17 00:00:00\n",
      "DONE: future promo data for 2017-05-17 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-05-10 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-05-10 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-05-10 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-05-10 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-05-10 00:00:00\n",
      "DONE: no sales diff in for 2017-05-10 00:00:00\n",
      "DONE: past promo avgs for 2017-05-10 00:00:00\n",
      "DONE: future promo data for 2017-05-10 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-05-03 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-05-03 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-05-03 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-05-03 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-05-03 00:00:00\n",
      "DONE: no sales diff in for 2017-05-03 00:00:00\n",
      "DONE: past promo avgs for 2017-05-03 00:00:00\n",
      "DONE: future promo data for 2017-05-03 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-04-26 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-04-26 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-04-26 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-04-26 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-04-26 00:00:00\n",
      "DONE: no sales diff in for 2017-04-26 00:00:00\n",
      "DONE: past promo avgs for 2017-04-26 00:00:00\n",
      "DONE: future promo data for 2017-04-26 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-04-19 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-04-19 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-04-19 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-04-19 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-04-19 00:00:00\n",
      "DONE: no sales diff in for 2017-04-19 00:00:00\n",
      "DONE: past promo avgs for 2017-04-19 00:00:00\n",
      "DONE: future promo data for 2017-04-19 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-04-12 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-04-12 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-04-12 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-04-12 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-04-12 00:00:00\n",
      "DONE: no sales diff in for 2017-04-12 00:00:00\n",
      "DONE: past promo avgs for 2017-04-12 00:00:00\n",
      "DONE: future promo data for 2017-04-12 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-04-05 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-04-05 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-04-05 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-04-05 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-04-05 00:00:00\n",
      "DONE: no sales diff in for 2017-04-05 00:00:00\n",
      "DONE: past promo avgs for 2017-04-05 00:00:00\n",
      "DONE: future promo data for 2017-04-05 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-03-29 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-03-29 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-03-29 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-03-29 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: avg count of item not sold in a store for 2017-03-29 00:00:00\n",
      "DONE: no sales diff in for 2017-03-29 00:00:00\n",
      "DONE: past promo avgs for 2017-03-29 00:00:00\n",
      "DONE: future promo data for 2017-03-29 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-03-22 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-03-22 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-03-22 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-03-22 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-03-22 00:00:00\n",
      "DONE: no sales diff in for 2017-03-22 00:00:00\n",
      "DONE: past promo avgs for 2017-03-22 00:00:00\n",
      "DONE: future promo data for 2017-03-22 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-03-15 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-03-15 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-03-15 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-03-15 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-03-15 00:00:00\n",
      "DONE: no sales diff in for 2017-03-15 00:00:00\n",
      "DONE: past promo avgs for 2017-03-15 00:00:00\n",
      "DONE: future promo data for 2017-03-15 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-03-08 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-03-08 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-03-08 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-03-08 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-03-08 00:00:00\n",
      "DONE: no sales diff in for 2017-03-08 00:00:00\n",
      "DONE: past promo avgs for 2017-03-08 00:00:00\n",
      "DONE: future promo data for 2017-03-08 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-03-01 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-03-01 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-03-01 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-03-01 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-03-01 00:00:00\n",
      "DONE: no sales diff in for 2017-03-01 00:00:00\n",
      "DONE: past promo avgs for 2017-03-01 00:00:00\n",
      "DONE: future promo data for 2017-03-01 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-02-22 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-02-22 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-02-22 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-02-22 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-02-22 00:00:00\n",
      "DONE: no sales diff in for 2017-02-22 00:00:00\n",
      "DONE: past promo avgs for 2017-02-22 00:00:00\n",
      "DONE: future promo data for 2017-02-22 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-02-15 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-02-15 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-02-15 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-02-15 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-02-15 00:00:00\n",
      "DONE: no sales diff in for 2017-02-15 00:00:00\n",
      "DONE: past promo avgs for 2017-02-15 00:00:00\n",
      "DONE: future promo data for 2017-02-15 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-02-08 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-02-08 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-02-08 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-02-08 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-02-08 00:00:00\n",
      "DONE: no sales diff in for 2017-02-08 00:00:00\n",
      "DONE: past promo avgs for 2017-02-08 00:00:00\n",
      "DONE: future promo data for 2017-02-08 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-02-01 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-02-01 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-02-01 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-02-01 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-02-01 00:00:00\n",
      "DONE: no sales diff in for 2017-02-01 00:00:00\n",
      "DONE: past promo avgs for 2017-02-01 00:00:00\n",
      "DONE: future promo data for 2017-02-01 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-01-25 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-01-25 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-01-25 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-01-25 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-01-25 00:00:00\n",
      "DONE: no sales diff in for 2017-01-25 00:00:00\n",
      "DONE: past promo avgs for 2017-01-25 00:00:00\n",
      "DONE: future promo data for 2017-01-25 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-01-18 00:00:00\n",
      "DONE: avg difference in sales calculated for 2017-01-18 00:00:00\n",
      "DONE: avg sales for each day of week calculated for 2017-01-18 00:00:00\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-01-18 00:00:00\n",
      "DONE: avg count of item not sold in a store for 2017-01-18 00:00:00\n",
      "DONE: no sales diff in for 2017-01-18 00:00:00\n",
      "DONE: past promo avgs for 2017-01-18 00:00:00\n",
      "DONE: future promo data for 2017-01-18 00:00:00\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-07-26\n",
      "DONE: avg difference in sales calculated for 2017-07-26\n",
      "DONE: avg sales for each day of week calculated for 2017-07-26\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-07-26\n",
      "DONE: avg count of item not sold in a store for 2017-07-26\n",
      "DONE: no sales diff in for 2017-07-26\n",
      "DONE: past promo avgs for 2017-07-26\n",
      "DONE: future promo data for 2017-07-26\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n",
      "DONE: last sales and avg sales calculated for 2017-08-16\n",
      "DONE: avg difference in sales calculated for 2017-08-16\n",
      "DONE: avg sales for each day of week calculated for 2017-08-16\n",
      "DONE: avg sales for each day calculated for a quarter and year for 2017-08-16\n",
      "DONE: avg count of item not sold in a store for 2017-08-16\n",
      "DONE: no sales diff in for 2017-08-16\n",
      "DONE: past promo avgs for 2017-08-16\n",
      "DONE: future promo data for 2017-08-16\n",
      "##############################\n",
      "\n",
      "\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "training_set = load_to_memory(\"../kaggle_favorita-grocery-sales/data/train.csv\")\n",
    "print('loded training to memory, memory use: {} GB'.format(round(sum(list(training_set.memory_usage(deep=True)))) / 1e9, 2))\n",
    "test_set = load_to_memory(\"../kaggle_favorita-grocery-sales/data/test.csv\", is_test=True)\n",
    "print('loded test to memory, memory use: {} GB'.format(round(sum(list(test_set.memory_usage(deep=True)))) / 1e9, 2))\n",
    "\n",
    "\n",
    "# prepare sales & promotion data to extract features from. Fixing missing values. Agree on multilvl idx.\n",
    "# looking in between 2014-04-01 : 2017-08-31\n",
    "idx_keys = [training_set.columns[1], training_set.columns[2], training_set.columns[0]]\n",
    "sales_set_y, sales_set_x, promotions_set = sales_and_promotion_table(training_set, test_set, idx_keys)\n",
    "\n",
    "# populate sales set with meta_data for groupby operations\n",
    "# day_of_week | day | days_of_no_sale | family | class | city | state | type | cluster\n",
    "# meta data lookup table for key: store_nbr | item_nbr -> \n",
    "sales_set_x, lookup = sales_meta(sales_set_x,\n",
    "                                 \"../kaggle_favorita-grocery-sales/data/items.csv\", \n",
    "                                 \"../kaggle_favorita-grocery-sales/data/stores.csv\")\n",
    "\n",
    "train_date = [pd.to_datetime('2017-07-05') - pd.Timedelta(days=7 * i) for i in range(25)]\n",
    "validation_date = '2017-07-26'\n",
    "test_date = '2017-08-16'\n",
    "# get data ready for features\n",
    "_training_x = [get_features(i) for i in train_date]\n",
    "_training_lables = [get_y(sales_set_y,i) for i in train_date]\n",
    "_training_x = [i.set_index(list(i.columns[[0,1]])) for i in _training_x]\n",
    "_validation_x = get_features(validation_date)\n",
    "_test_x = get_features(test_date)\n",
    "_validation_lables = get_y(sales_set_y, validation_date)\n",
    "_validation_x = _validation_x.set_index(list(_validation_x.columns[[0,1]]) )\n",
    "_test_x = _test_x.set_index (list(_test_x.columns[[0,1]]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb2a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sup. data\n",
    "_items = pd.read_csv(\"../kaggle_favorita-grocery-sales/data/items.csv\")\n",
    "_stores = pd.read_csv(\"../kaggle_favorita-grocery-sales/data/stores.csv\")\n",
    "\n",
    "# encode metadata .. This is important as these are the 'keys'\n",
    "# these keys will later be used to find values that corresponding values that will point to the trainable embbedings  \n",
    "le = LabelEncoder()\n",
    "_items.family = le.fit_transform(_items.family)\n",
    "_stores.city = le.fit_transform(_stores.city)\n",
    "_stores.state = le.fit_transform(_stores.state)\n",
    "_stores.type = le.fit_transform(_stores.type)\n",
    "\n",
    "# look up table for meta\n",
    "items2 = _items.set_index('item_nbr').reindex(_validation_x.index.get_level_values(1))\n",
    "stores2 = _stores.set_index('store_nbr').reindex(_validation_x.index.get_level_values(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e074c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//////////////////////////////////////////////////\n",
      "Step 0\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.0891 - val_loss: 0.0777\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.0787 - val_loss: 0.0778\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 65s 8ms/step - loss: 0.0782 - val_loss: 0.0776\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 66s 8ms/step - loss: 0.0779 - val_loss: 0.0773\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 66s 8ms/step - loss: 0.0777 - val_loss: 0.0773\n",
      "Epoch 6/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0776 - val_loss: 0.0771\n",
      "Epoch 7/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.0774 - val_loss: 0.0771\n",
      "Epoch 8/16\n",
      "8500/8500 [==============================] - 67s 8ms/step - loss: 0.0773 - val_loss: 0.0769\n",
      "Epoch 9/16\n",
      "8500/8500 [==============================] - 66s 8ms/step - loss: 0.0772 - val_loss: 0.0778\n",
      "Epoch 10/16\n",
      "8500/8500 [==============================] - 63s 7ms/step - loss: 0.0771 - val_loss: 0.0770\n",
      "Epoch 11/16\n",
      "8500/8500 [==============================] - 63s 7ms/step - loss: 0.0770 - val_loss: 0.0770\n",
      "Epoch 00011: early stopping\n",
      "nwrmsle 0.27576\n",
      "time elapsed 750.9870422999998\n",
      "//////////////////////////////////////////////////\n",
      "Step 1\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 62s 7ms/step - loss: 0.0959 - val_loss: 0.0893\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 61s 7ms/step - loss: 0.0885 - val_loss: 0.0890\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 62s 7ms/step - loss: 0.0879 - val_loss: 0.0887\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 61s 7ms/step - loss: 0.0876 - val_loss: 0.0894\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 61s 7ms/step - loss: 0.0874 - val_loss: 0.0890\n",
      "Epoch 6/16\n",
      "8500/8500 [==============================] - 62s 7ms/step - loss: 0.0872 - val_loss: 0.0891\n",
      "Epoch 00006: early stopping\n",
      "nwrmsle 0.29772\n",
      "time elapsed 390.28740149999976\n",
      "//////////////////////////////////////////////////\n",
      "Step 2\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 64s 7ms/step - loss: 0.0926 - val_loss: 0.0890\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 63s 7ms/step - loss: 0.0859 - val_loss: 0.0890\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 63s 7ms/step - loss: 0.0853 - val_loss: 0.0887\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 64s 8ms/step - loss: 0.0849 - val_loss: 0.0893\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 67s 8ms/step - loss: 0.0847 - val_loss: 0.0888\n",
      "Epoch 6/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0845 - val_loss: 0.0888\n",
      "Epoch 00006: early stopping\n",
      "nwrmsle 0.29702\n",
      "time elapsed 411.9650907\n",
      "//////////////////////////////////////////////////\n",
      "Step 3\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0928 - val_loss: 0.0904\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0862 - val_loss: 0.0887\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0855 - val_loss: 0.0898\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0851 - val_loss: 0.0899\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0848 - val_loss: 0.0881\n",
      "Epoch 6/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0845 - val_loss: 0.0886\n",
      "Epoch 7/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.0843 - val_loss: 0.0902\n",
      "Epoch 8/16\n",
      "8500/8500 [==============================] - 65s 8ms/step - loss: 0.0842 - val_loss: 0.0874\n",
      "Epoch 9/16\n",
      "8500/8500 [==============================] - 66s 8ms/step - loss: 0.0840 - val_loss: 0.0879\n",
      "Epoch 10/16\n",
      "8500/8500 [==============================] - 65s 8ms/step - loss: 0.0839 - val_loss: 0.0886\n",
      "Epoch 11/16\n",
      "8500/8500 [==============================] - 65s 8ms/step - loss: 0.0837 - val_loss: 0.0882\n",
      "Epoch 00011: early stopping\n",
      "nwrmsle 0.29504\n",
      "time elapsed 764.6626754000004\n",
      "//////////////////////////////////////////////////\n",
      "Step 4\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 67s 8ms/step - loss: 0.0949 - val_loss: 0.0883\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 63s 7ms/step - loss: 0.0878 - val_loss: 0.0876\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 63s 7ms/step - loss: 0.0870 - val_loss: 0.0873\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 63s 7ms/step - loss: 0.0865 - val_loss: 0.0868\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 63s 7ms/step - loss: 0.0861 - val_loss: 0.0873\n",
      "Epoch 6/16\n",
      "8500/8500 [==============================] - 65s 8ms/step - loss: 0.0859 - val_loss: 0.0872\n",
      "Epoch 7/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0857 - val_loss: 0.0872\n",
      "Epoch 00007: early stopping\n",
      "nwrmsle 0.29379\n",
      "time elapsed 474.1856289999996\n",
      "//////////////////////////////////////////////////\n",
      "Step 5\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 71s 8ms/step - loss: 0.1022 - val_loss: 0.0941\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.0933 - val_loss: 0.0931\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0925 - val_loss: 0.0938\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 72s 8ms/step - loss: 0.0920 - val_loss: 0.0924\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 73s 9ms/step - loss: 0.0917 - val_loss: 0.0934\n",
      "Epoch 6/16\n",
      "8500/8500 [==============================] - 73s 9ms/step - loss: 0.0915 - val_loss: 0.0926\n",
      "Epoch 7/16\n",
      "8500/8500 [==============================] - 73s 9ms/step - loss: 0.0912 - val_loss: 0.0928\n",
      "Epoch 00007: early stopping\n",
      "nwrmsle 0.30359\n",
      "time elapsed 521.1881376000001\n",
      "//////////////////////////////////////////////////\n",
      "Step 6\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 71s 8ms/step - loss: 0.0974 - val_loss: 0.1003\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 71s 8ms/step - loss: 0.0920 - val_loss: 0.0982\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0914 - val_loss: 0.0979\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 71s 8ms/step - loss: 0.0910 - val_loss: 0.0959\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0907 - val_loss: 0.0958\n",
      "Epoch 6/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0905 - val_loss: 0.0977\n",
      "Epoch 7/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0904 - val_loss: 0.0964\n",
      "Epoch 8/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0902 - val_loss: 0.0963\n",
      "Epoch 00008: early stopping\n",
      "nwrmsle 0.30888\n",
      "time elapsed 581.6140291000002\n",
      "//////////////////////////////////////////////////\n",
      "Step 7\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0998 - val_loss: 0.0899\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.0878 - val_loss: 0.0899\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 71s 8ms/step - loss: 0.0871 - val_loss: 0.0900\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.0868 - val_loss: 0.0892\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.0865 - val_loss: 0.0909\n",
      "Epoch 6/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.0862 - val_loss: 0.0911\n",
      "Epoch 7/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0860 - val_loss: 0.0897\n",
      "Epoch 00007: early stopping\n",
      "nwrmsle 0.29678\n",
      "time elapsed 503.1464808999999\n",
      "//////////////////////////////////////////////////\n",
      "Step 8\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.1010 - val_loss: 0.0977\n",
      "Epoch 2/16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0952 - val_loss: 0.0979\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0945 - val_loss: 0.0985\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.0941 - val_loss: 0.0976\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0938 - val_loss: 0.0964\n",
      "Epoch 6/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0935 - val_loss: 0.0972\n",
      "Epoch 7/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.0933 - val_loss: 0.0966\n",
      "Epoch 8/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0931 - val_loss: 0.0967\n",
      "Epoch 00008: early stopping\n",
      "nwrmsle 0.30967\n",
      "time elapsed 572.4697794000003\n",
      "//////////////////////////////////////////////////\n",
      "Step 9\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.1020 - val_loss: 0.0950\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 67s 8ms/step - loss: 0.0924 - val_loss: 0.0930\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 67s 8ms/step - loss: 0.0916 - val_loss: 0.0934\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0912 - val_loss: 0.0930\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0908 - val_loss: 0.0932\n",
      "Epoch 00005: early stopping\n",
      "nwrmsle 0.30309\n",
      "time elapsed 360.6782394999991\n",
      "//////////////////////////////////////////////////\n",
      "Step 10\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.1026 - val_loss: 0.0942\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0936 - val_loss: 0.0939\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0926 - val_loss: 0.0940\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0920 - val_loss: 0.0936\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0916 - val_loss: 0.0938\n",
      "Epoch 6/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0912 - val_loss: 0.0938\n",
      "Epoch 7/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0909 - val_loss: 0.0939\n",
      "Epoch 00007: early stopping\n",
      "nwrmsle 0.30474\n",
      "time elapsed 502.9193885000004\n",
      "//////////////////////////////////////////////////\n",
      "Step 11\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 71s 8ms/step - loss: 0.1031 - val_loss: 0.0953\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0945 - val_loss: 0.0948\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0935 - val_loss: 0.0946\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0929 - val_loss: 0.0949\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0925 - val_loss: 0.0947\n",
      "Epoch 6/16\n",
      "8500/8500 [==============================] - 66s 8ms/step - loss: 0.0921 - val_loss: 0.0946\n",
      "Epoch 7/16\n",
      "8500/8500 [==============================] - 67s 8ms/step - loss: 0.0918 - val_loss: 0.0950\n",
      "Epoch 8/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.0915 - val_loss: 0.0949\n",
      "Epoch 9/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0913 - val_loss: 0.0952\n",
      "Epoch 00009: early stopping\n",
      "nwrmsle 0.30615\n",
      "time elapsed 639.8959723999997\n",
      "//////////////////////////////////////////////////\n",
      "Step 12\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 65s 8ms/step - loss: 0.1073 - val_loss: 0.0991\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 63s 7ms/step - loss: 0.0984 - val_loss: 0.0983\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 63s 7ms/step - loss: 0.0975 - val_loss: 0.0986\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 63s 7ms/step - loss: 0.0969 - val_loss: 0.0983\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 66s 8ms/step - loss: 0.0966 - val_loss: 0.0980\n",
      "Epoch 6/16\n",
      "8500/8500 [==============================] - 66s 8ms/step - loss: 0.0962 - val_loss: 0.0984\n",
      "Epoch 7/16\n",
      "8500/8500 [==============================] - 66s 8ms/step - loss: 0.0960 - val_loss: 0.0984\n",
      "Epoch 8/16\n",
      "8500/8500 [==============================] - 67s 8ms/step - loss: 0.0957 - val_loss: 0.0984\n",
      "Epoch 00008: early stopping\n",
      "nwrmsle 0.31237\n",
      "time elapsed 539.9078129000009\n",
      "//////////////////////////////////////////////////\n",
      "Step 13\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 67s 8ms/step - loss: 0.1071 - val_loss: 0.0966\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 66s 8ms/step - loss: 0.0967 - val_loss: 0.0967\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 66s 8ms/step - loss: 0.0959 - val_loss: 0.0966\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0955 - val_loss: 0.0969\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0951 - val_loss: 0.0963\n",
      "Epoch 6/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0948 - val_loss: 0.0962\n",
      "Epoch 7/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0946 - val_loss: 0.0969\n",
      "Epoch 8/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.0944 - val_loss: 0.0974\n",
      "Epoch 9/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0942 - val_loss: 0.0967\n",
      "Epoch 00009: early stopping\n",
      "nwrmsle 0.30910\n",
      "time elapsed 638.3078698999998\n",
      "//////////////////////////////////////////////////\n",
      "Step 14\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.1015 - val_loss: 0.0925\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.0923 - val_loss: 0.0927\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 68s 8ms/step - loss: 0.0915 - val_loss: 0.0932\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 69s 8ms/step - loss: 0.0910 - val_loss: 0.0929\n",
      "Epoch 00004: early stopping\n",
      "nwrmsle 0.30206\n",
      "time elapsed 296.5369274000004\n",
      "//////////////////////////////////////////////////\n",
      "Step 15\n",
      "//////////////////////////////////////////////////\n",
      "Epoch 1/16\n",
      "8500/8500 [==============================] - 70s 8ms/step - loss: 0.1060 - val_loss: 0.1014\n",
      "Epoch 2/16\n",
      "8500/8500 [==============================] - 67s 8ms/step - loss: 0.0997 - val_loss: 0.1002\n",
      "Epoch 3/16\n",
      "8500/8500 [==============================] - 67s 8ms/step - loss: 0.0989 - val_loss: 0.1007\n",
      "Epoch 4/16\n",
      "8500/8500 [==============================] - 67s 8ms/step - loss: 0.0983 - val_loss: 0.1004\n",
      "Epoch 5/16\n",
      "8500/8500 [==============================] - 67s 8ms/step - loss: 0.0979 - val_loss: 0.1007\n",
      "Epoch 00005: early stopping\n",
      "nwrmsle 0.31572\n",
      "time elapsed 360.43645209999886\n"
     ]
    }
   ],
   "source": [
    "# training / evaluation block\n",
    "pred = []\n",
    "test_pred = []\n",
    "\n",
    "# for the 16 days to be predicted\n",
    "for i in range(16):\n",
    "\n",
    "    print(\"/\" * 50)\n",
    "    print(\"Step %d\" % (i))\n",
    "    print(\"/\" * 50)\n",
    "    start_timer = timeit.default_timer()\n",
    "\n",
    "    # get features for validation\n",
    "    val, yval = _features(_validation_x, i, _validation_lables)\n",
    "    # get features for testing\n",
    "    test = _features(_test_x, i)\n",
    "\n",
    "    train = []\n",
    "    ytrain = []\n",
    "    for j in range(len(_training_x)):\n",
    "        tr, ytr = _features(_training_x[j], i,_training_lables[j])\n",
    "        train.append(tr)\n",
    "        ytrain.append(ytr)\n",
    "    train = pd.concat(train)\n",
    "    ytrain = pd.concat(ytrain)\n",
    "    ########################################################################################################\n",
    "    train_set = [train.drop(['store_nbr', 'item_nbr', 'family', 'class', 'city', 'state', 'type', 'cluster',\n",
    "                             'day_of_week', 'day'], axis=1).values,\n",
    "                train.store_nbr.values, train.item_nbr.values,\n",
    "                train.family.values,\n",
    "                train.city.values, train.state.values, train.type.values, train.cluster.values,\n",
    "                train.day_of_week.values, train.day.values]\n",
    "    val_set = [val.drop(['store_nbr', 'item_nbr', 'family', 'class', 'city', 'state', 'type', 'cluster',\n",
    "                             'day_of_week', 'day'], axis=1).values,\n",
    "                val.store_nbr.values, val.item_nbr.values,\n",
    "                val.family.values,\n",
    "                val.city.values, val.state.values, val.type.values, val.cluster.values,\n",
    "                val.day_of_week.values, val.day.values]\n",
    "    test_set = [test.drop(['store_nbr', 'item_nbr', 'family', 'class', 'city', 'state', 'type', 'cluster',\n",
    "                             'day_of_week', 'day'], axis=1).values,\n",
    "                test.store_nbr.values, test.item_nbr.values,\n",
    "                test.family.values,\n",
    "                test.city.values, test.state.values, test.type.values, test.cluster.values,\n",
    "                test.day_of_week.values, test.day.values]\n",
    "\n",
    "    gc.collect()\n",
    "    ########################################################################################################\n",
    "\n",
    "    model = get_model(train.shape[1]-10) # -10 due to 10 embedding features / shape is 30 for Coles interview\n",
    "\n",
    "    earlyStopping=EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\n",
    "    checkpointer = ModelCheckpoint(filepath=\"tmp/weights.h5\", verbose=0, save_best_only=True,\n",
    "                               save_weights_only=True)\n",
    "\n",
    "    model.fit(train_set, np.log(1+ytrain.values),\n",
    "            validation_data=(val_set, np.log(1+yval.values)),\n",
    "            epochs=16, batch_size=512, verbose=1, callbacks=[tensorboard_callback,earlyStopping, checkpointer])\n",
    "\n",
    "    model.load_weights('tmp/weights.h5')\n",
    "    pred.append(np.exp(model.predict(val_set))-1)\n",
    "    test_pred.append(np.exp(model.predict(test_set))-1)\n",
    "\n",
    "    print('nwrmsle %.5f' % nwrmsle(yval.values, pred[-1], weights=val.perishable.values*0.25+1))\n",
    "    print('time elapsed', timeit.default_timer()-start_timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd3a825d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5939973266858016"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation\n",
    "nwrmsle(np.expm1(np.array(pred).transpose().squeeze()), np.expm1(_validation_lables.values), weights=items2.perishable.values*0.25+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2928feac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22505914,  0.16737652,  0.29552162, ...,  0.13525485,\n",
       "         0.19792454,  0.26601425],\n",
       "       [ 0.29353425,  0.2980149 ,  0.39999387, ...,  0.28511173,\n",
       "         0.38257676,  0.30394757],\n",
       "       [ 1.1943672 ,  0.9658501 ,  1.0325513 , ...,  0.9617865 ,\n",
       "         1.0536716 ,  0.89230216],\n",
       "       ...,\n",
       "       [11.330066  , 11.726071  ,  9.059416  , ...,  8.43863   ,\n",
       "         6.0986266 ,  4.4603643 ],\n",
       "       [ 0.5755954 ,  0.33560085,  0.5506027 , ...,  0.36428055,\n",
       "         0.41199687,  0.49086285],\n",
       "       [ 2.427124  ,  0.01861484,  0.06373626, ...,  0.07948102,\n",
       "         2.189936  ,  0.09063169]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expm1(np.array(test_pred).transpose().squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (rabbit)",
   "language": "python",
   "name": "rabbit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
